{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.conda/envs/trash_demo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, PaliGemmaForConditionalGeneration\n",
    "import PIL\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "from training_toolkit.common.tokenization_utils.segmentation import (\n",
    "    SegmentationTokenizer,\n",
    ")\n",
    "\n",
    "from training_toolkit.common.tokenization_utils.json import (\n",
    "    JSONTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"google/paligemma-3b-mix-224\"\n",
    "SEG_CHECKPOINT_PATH = \"paligemma_2024-08-06_09-05-06\"\n",
    "JSON_CHECKPOINT_PATH = \"paligemma_2024-08-13_07-21-14/checkpoint-240\"\n",
    "\n",
    "\n",
    "base_model = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, SEG_CHECKPOINT_PATH, adapter_name=\"segmentation\")\n",
    "\n",
    "# load different adapter\n",
    "model.load_adapter(JSON_CHECKPOINT_PATH, adapter_name=\"json\")\n",
    "\n",
    "# set adapter as active\n",
    "# model.set_adapter(\"json\")\n",
    "\n",
    "segmentation_tokenizer = SegmentationTokenizer()\n",
    "json_tokenizer = JSONTokenizer(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(\"../assets/trash1.jpg\")\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    \"carton\",\n",
    "    \"foam\",\n",
    "    \"food\",\n",
    "    \"general\",\n",
    "    \"glass\",\n",
    "    \"metal\",\n",
    "    \"paper\",\n",
    "    \"plastic\",\n",
    "    \"special\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"segment \" + \" ; \".join(class_names)\n",
    "\n",
    "inputs = processor(images=image, text=PROMPT)\n",
    "\n",
    "model.set_adapter(\"segmentation\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "\n",
    "# Next we turn each predicted token ID back into a string using the decode method\n",
    "# We chop of the prompt, which consists of image tokens and our text prompt\n",
    "image_token_index = model.config.image_token_index\n",
    "num_image_tokens = len(generated_ids[generated_ids == image_token_index])\n",
    "num_text_tokens = len(processor.tokenizer.encode(PROMPT))\n",
    "num_prompt_tokens = num_image_tokens + num_text_tokens + 2\n",
    "generated_text = processor.batch_decode(\n",
    "    generated_ids[:, num_prompt_tokens:],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")[0]\n",
    "\n",
    "w, h = image.size\n",
    "\n",
    "generated_segmentation = segmentation_tokenizer.decode(generated_text, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '<loc0207><loc0536><loc0792><loc0898> <seg015><seg066><seg066><seg088><seg022><seg091><seg044><seg022><seg104><seg048><seg078><seg026><seg072><seg095><seg075><seg026> paper ; ',\n",
       "  'xyxy': (1708, 495, 2862, 1893),\n",
       "  'mask': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  'name': 'paper '},\n",
       " {'content': '<loc0210><loc0364><loc0706><loc0529> <seg055><seg011><seg059><seg048><seg030><seg012><seg119><seg082><seg026><seg042><seg061><seg030><seg007><seg075><seg068><seg030> special ; ',\n",
       "  'xyxy': (1160, 502, 1686, 1688),\n",
       "  'mask': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  'name': 'special '},\n",
       " {'content': '<loc0352><loc0635><loc0457><loc0755><seg097><seg089><seg039><seg124><seg049><seg063><seg091><seg055><seg083><seg098><seg017><seg113><seg096><seg051><seg094><seg002> general ; ',\n",
       "  'xyxy': (2024, 842, 2407, 1093),\n",
       "  'mask': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  'name': 'general '},\n",
       " {'content': '<loc0209><loc0343><loc0716><loc0515> <seg092><seg074><seg080><seg029><seg079><seg012><seg090><seg120><seg066><seg064><seg082><seg022><seg025><seg073><seg054><seg000> special',\n",
       "  'xyxy': (1093, 500, 1642, 1712),\n",
       "  'mask': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  'name': 'special'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\n",
    "    (0, 255, 255),  # Cyan\n",
    "    (255, 128, 128),  # Salmon\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (255, 128, 0),  # Orange\n",
    "    (128, 255, 0),  # Lime\n",
    "    (0, 255, 128),  # Spring Green\n",
    "    (255, 0, 128),  # Rose\n",
    "    (128, 0, 255),  # Violet\n",
    "    (0, 128, 255),  # Azure\n",
    "    (128, 255, 128),  # Chartreuse\n",
    "    (128, 128, 255),  # Cornflower Blue\n",
    "    (255, 255, 128),  # Light Yellow\n",
    "    (255, 128, 255),  # Orchid\n",
    "    (128, 255, 255),  # Light Cyan\n",
    "    (255, 165, 0),  # Also orange\n",
    "    (0, 255, 255),  # Aqua\n",
    "    (255, 0, 255),  # Fuchsia\n",
    "    (128, 0, 0),  # Maroon\n",
    "    (128, 128, 0),  # Olive\n",
    "    (0, 128, 128),  # Teal\n",
    "    (128, 0, 128),  # Purple\n",
    "]\n",
    "\n",
    "colors = itertools.cycle(COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_contours(image, masks):\n",
    "    for j, mask in enumerate(masks):\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        image = cv2.drawContours(image, contours, -1, next(colors), 3)\n",
    "\n",
    "        if not contours:\n",
    "            continue\n",
    "\n",
    "        # Calculate the center of the outline\n",
    "        M = cv2.moments(contours[0])\n",
    "\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "        # Add a backdrop to the label for visibility\n",
    "        # text_size, _ = cv2.getTextSize(str(0), cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (cX - 5, cY - 30),\n",
    "            (cX + 30, cY + 5),\n",
    "            (0, 0, 0),  # Black color for the backdrop\n",
    "            -1,  # Fill the rectangle\n",
    "        )\n",
    "\n",
    "        # Draw the label with the index of the mask in the middle of the outline\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            str(j),  # Replace 0 with the index of the mask\n",
    "            (cX, cY),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),  # White color for the text\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2448, 3264)\n",
      "(2448, 3264)\n",
      "(2448, 3264)\n",
      "(2448, 3264)\n"
     ]
    }
   ],
   "source": [
    "for seg in generated_segmentation:\n",
    "    print(seg[\"mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = [(seg[\"mask\"] > 0.5).astype(np.uint8) * 255 for seg in generated_segmentation]\n",
    "\n",
    "image_with_masks = draw_contours(np.array(image), masks)\n",
    "image_with_masks = PIL.Image.fromarray(image_with_masks)\n",
    "image_with_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For every object outlined in the image, here\\'re their detected classes: [{\\'item_id\\': 0, \\'class\\': \\'paper\\'}, {\\'item_id\\': 1, \\'class\\': \\'special\\'}, {\\'item_id\\': 2, \\'class\\': \\'general\\'}, {\\'item_id\\': 3, \\'class\\': \\'special\\'}]. For every outlined item, extract JSON with a more accurate label, as well as disposal directions based on local rules. The local rules are as follows: In this community, garbage sorting rules are as follows:\\n\\n### General Waste Bag (일반 쓰레기 봉투)\\n- **Color:** Typically white or green (varies by district).\\n- **Contents:** Everything not classified as recyclable or food waste. Examples include used tissues, used toilet paper (when not flushed), sanitary pads, old shoes, and clothes.\\n\\n### Food Waste Bag (음식물 쓰레기 봉투)\\n- **Contents:** Edible waste such as fruit peels, vegetable peels, uneaten meat, and raw eggs (without the shell).\\n- **Exceptions:** Egg shells, crustacean shells, clam shells, onion and garlic skin, animal bones, and tea bags/leaves are not considered food waste.\\n\\n### Recyclable Waste (재활용 쓰레기)\\n- **Containers/Bags:** No special bags required; use designated bins or any plastic bag if bins are not provided.\\n- **Types:** \\n  - Paper Pack (juice, milk cartons)\\n  - Glass (beer bottles)\\n  - Cans (aluminum or iron)\\n  - Paper (printing paper, magazines, newspapers, boxes)\\n  - Plastic (excluding toys, pens, small candy wraps)\\n  - PET Bottles\\n  - Vinyl (snack bags, dessert wraps)\\n\\n### Oversized Waste (대형 쓰레기)\\n- **Examples:** Refrigerators, induction cookers, TVs, air conditioners, big furniture (mattresses, sofas, desks).\\n- **Procedure:** Contact the local district office to arrange for disposal. Obtain a disposal sticker after providing details about the item. Place the item in the designated area on the appointed date.\\n\\n### Buying Garbage Bags\\n- Available in bulk at convenience stores, supermarkets, or neighborhood marts.\\n- Phrases for asking: \\n  - \"일반 쓰레기 봉투 있어요?\" (Do you have general waste garbage bags?)\\n  - \"음식물 쓰레기 봉투 있어요?\" (Do you have food waste garbage bags?)\\n\\n### Collection Time & Place\\n- **Collection Times:** Typically after sundown (around 7:00 PM) until early morning (5:00 AM).\\n- **Location:** Varies by district. Check with the landlord, building management, or on the garbage bags for specific details.\\n\\n### Penalties\\n- Non-compliance can result in fines up to ₩300,000 (approximately USD $300).\\n\\nThese rules aim to promote effective waste management and reduce the carbon footprint in the community..'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import Template\n",
    "\n",
    "with open(\"korea_summary.txt\", \"r\") as f:\n",
    "    rules = f.read()\n",
    "\n",
    "items = [{\"item_id\": i, \"class\": seg[\"name\"].strip()} for i, seg in enumerate(generated_segmentation)]\n",
    "\n",
    "# dataset = load_from_disk(\"taco_trash_json\")\n",
    "# image = dataset[0][\"image\"]\n",
    "# image\n",
    "\n",
    "# rules = dataset[0][\"prefix\"][\"rules\"]\n",
    "# items = dataset[0][\"prefix\"][\"items\"]\n",
    "\n",
    "PREFIX_TEMPLATE = Template(\"For every object outlined in the image, here're their detected classes: $items. \"\n",
    "                           \"For every outlined item, extract JSON with a more accurate label, \"\n",
    "                           \"as well as disposal directions based on local rules. \"\n",
    "                           \"The local rules are as follows: $rules.\")\n",
    "\n",
    "prompt = PREFIX_TEMPLATE.substitute(items=items, rules=rules)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_adapter(\"json\")\n",
    "\n",
    "inputs = processor(images=image_with_masks, text=prompt, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# Next we turn each predicted token ID back into a string using the decode method\n",
    "# We chop of the prompt, which consists of image tokens and our text prompt\n",
    "\n",
    "generated_text = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")[0]\n",
    "\n",
    "generated_json = json_tokenizer.decode(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sorted_items': [{'refined_label': 'Silk',\n",
       "    'item_id': '0',\n",
       "    'item_class': 'Food Waste Bag',\n",
       "    'direction': 'Recyclable Waste (재활용 쓰레기) (No special bags required)'},\n",
       "   {'refined_label': 'Straws',\n",
       "    'item_id': '2',\n",
       "    'item_class': 'Food Waste Bag',\n",
       "    'direction': 'Recyclable Waste (재활용 쓰레기) (No special bags required)',\n",
       "    'additional_label': 'Recyclable Waste (재활용 쓰레기) (No special bags required)',\n",
       "    'label': 'Special Gifts'},\n",
       "   {'refined_label': 'Silk',\n",
       "    'item_id': '4',\n",
       "    'item_class': 'Food Waste Bag',\n",
       "    'direction': 'Recyclable Waste (재활용 쓰레기) (No special bags required)',\n",
       "    'additional_label': 'Recyclable Waste (재활용 쓰레기) (No special bags required)',\n",
       "    'label': 'Special Gifts'},\n",
       "   {'refined_label': 'Silk',\n",
       "    'item_id': '5',\n",
       "    'item_class': 'Food Waste Bag',\n",
       "    'direction': 'Recyclable Waste (재활용 쓰레기) (No special bags required)'},\n",
       "   {'refined_label': 'Silk',\n",
       "    'item_id': '6',\n",
       "    'item_class': 'Food Waste Bag',\n",
       "    'direction': 'Recyclable Waste (재활용 쓰레기) (No special bags required)'}],\n",
       "  'description': 'Issues with recycling in South Korea. Learn more.'},\n",
       " {'description': 'Make sure to put items in appropriate garbage bags. Garbage bags are color-coded and district-specific. You can obtain appropriate garbage bags at local stores.'},\n",
       " {'description': 'Failure to comply with district-specific recycling rules can result in a fine'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
