{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦¾ Training Toolkit: Multi-adapter inference\n",
    "\n",
    "It's time for us to load both of our adapters along with the base model and put together an inference pipeline.\n",
    "\n",
    "## 1. Load adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "import PIL\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "from training_toolkit.common.tokenization_utils.segmentation import (\n",
    "    SegmentationTokenizer,\n",
    ")\n",
    "\n",
    "from training_toolkit.common.tokenization_utils.json import (\n",
    "    JSONTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/paligemma-3b-mix-224\"\n",
    "SEG_CHECKPOINT_PATH = \"paligemma_trash_segm_adapter\"\n",
    "JSON_CHECKPOINT_PATH = \"paligemma_trash_json_adapter\"\n",
    "\n",
    "# 1. Load the base model straight from the hub\n",
    "base_model = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# 2. Load the first adapter\n",
    "model = PeftModel.from_pretrained(base_model, SEG_CHECKPOINT_PATH, adapter_name=\"segmentation\")\n",
    "\n",
    "# 3. Load the second adapter\n",
    "model.load_adapter(JSON_CHECKPOINT_PATH, adapter_name=\"json\")\n",
    "\n",
    "# 4. Prepare utility classes to process inputs and outputs\n",
    "segmentation_tokenizer = SegmentationTokenizer()\n",
    "json_tokenizer = JSONTokenizer(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(\"../assets/trash1.jpg\")\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coming straight from the segmentation notebook\n",
    "\n",
    "class_names = {\n",
    "    \"carton\",\n",
    "    \"foam\",\n",
    "    \"food\",\n",
    "    \"general\",\n",
    "    \"glass\",\n",
    "    \"metal\",\n",
    "    \"paper\",\n",
    "    \"plastic\",\n",
    "    \"special\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Do segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare segmentation inputs\n",
    "\n",
    "PROMPT = \"segment \" + \" ; \".join(class_names)\n",
    "inputs = processor(images=image, text=PROMPT)\n",
    "\n",
    "# Enable segmentation adapter\n",
    "model.set_adapter(\"segmentation\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "\n",
    "# Post process segmentation outputs\n",
    "image_token_index = model.config.image_token_index\n",
    "num_image_tokens = len(generated_ids[generated_ids == image_token_index])\n",
    "num_text_tokens = len(processor.tokenizer.encode(PROMPT))\n",
    "num_prompt_tokens = num_image_tokens + num_text_tokens + 2\n",
    "\n",
    "generated_text = processor.batch_decode(\n",
    "    generated_ids[:, num_prompt_tokens:],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")[0]\n",
    "\n",
    "w, h = image.size\n",
    "\n",
    "# Reconstruct the segmentation mask\n",
    "generated_segmentation = segmentation_tokenizer.decode(generated_text, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post process segmentation mask to create JSON inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\n",
    "    (0, 255, 255),  # Cyan\n",
    "    (255, 128, 128),  # Salmon\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (255, 128, 0),  # Orange\n",
    "    (128, 255, 0),  # Lime\n",
    "    (0, 255, 128),  # Spring Green\n",
    "    (255, 0, 128),  # Rose\n",
    "    (128, 0, 255),  # Violet\n",
    "    (0, 128, 255),  # Azure\n",
    "    (128, 255, 128),  # Chartreuse\n",
    "    (128, 128, 255),  # Cornflower Blue\n",
    "    (255, 255, 128),  # Light Yellow\n",
    "    (255, 128, 255),  # Orchid\n",
    "    (128, 255, 255),  # Light Cyan\n",
    "    (255, 165, 0),  # Also orange\n",
    "    (0, 255, 255),  # Aqua\n",
    "    (255, 0, 255),  # Fuchsia\n",
    "    (128, 0, 0),  # Maroon\n",
    "    (128, 128, 0),  # Olive\n",
    "    (0, 128, 128),  # Teal\n",
    "    (128, 0, 128),  # Purple\n",
    "]\n",
    "\n",
    "colors = itertools.cycle(COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_contours(image, masks):\n",
    "    for j, mask in enumerate(masks):\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        image = cv2.drawContours(image, contours, -1, next(colors), 3)\n",
    "\n",
    "        if not contours:\n",
    "            continue\n",
    "\n",
    "        # Calculate the center of the outline\n",
    "        M = cv2.moments(contours[0])\n",
    "\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "        # Add a backdrop to the label for visibility\n",
    "        # text_size, _ = cv2.getTextSize(str(0), cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (cX - 5, cY - 30),\n",
    "            (cX + 30, cY + 5),\n",
    "            (0, 0, 0),  # Black color for the backdrop\n",
    "            -1,  # Fill the rectangle\n",
    "        )\n",
    "\n",
    "        # Draw the label with the index of the mask in the middle of the outline\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            str(j),  # Replace 0 with the index of the mask\n",
    "            (cX, cY),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),  # White color for the text\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the contours on the image\n",
    "\n",
    "masks = [(seg[\"mask\"] > 0.5).astype(np.uint8) * 255 for seg in generated_segmentation]\n",
    "\n",
    "image_with_masks = draw_contours(np.array(image), masks)\n",
    "image_with_masks = PIL.Image.fromarray(image_with_masks)\n",
    "image_with_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Do JSON extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['name'] for x in generated_segmentation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "with open(\"korea_summary.txt\", \"r\") as f:\n",
    "    rules = f.read()\n",
    "\n",
    "items = [\n",
    "    {\"item_id\": i, \"class\": seg[\"name\"].strip()}\n",
    "    for i, seg in enumerate(generated_segmentation)\n",
    "    if seg[\"name\"]\n",
    "]\n",
    "\n",
    "PREFIX_TEMPLATE = Template(\n",
    "    \"For every object outlined in the image, here're their detected classes: $items. \"\n",
    "    \"For every outlined item, extract JSON with a more accurate label, \"\n",
    "    \"as well as disposal directions based on local rules. \"\n",
    "    \"The local rules are as follows: $rules.\"\n",
    ")\n",
    "\n",
    "prompt = PREFIX_TEMPLATE.substitute(items=items, rules=rules)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the JSON adapter\n",
    "model.set_adapter(\"json\")\n",
    "\n",
    "inputs = processor(images=image_with_masks, text=prompt, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "generated_text = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")[0]\n",
    "\n",
    "generated_json = json_tokenizer.decode(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
