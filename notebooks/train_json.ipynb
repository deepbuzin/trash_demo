{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦¾ Training Toolkit: Structured output\n",
    "\n",
    "## 1. Generate synthetic data\n",
    "\n",
    "### Prepare GPT-4o annotations generator\n",
    "\n",
    "We're goint to build a LangChain structured output chain in order to generate target JSONs for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional, List\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the structured schema\n",
    "\n",
    "class TrashItem(BaseModel):\n",
    "    \"\"\"\n",
    "    A trash item that needs to be sorted into a category.\n",
    "    \"\"\"\n",
    "\n",
    "    item_id: Optional[str] = Field(description=\"The unique identifier of the item.\")\n",
    "    direction: Optional[str] = Field(\n",
    "        description=\"How to dispose of the item according to the rules.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Quirk(BaseModel):\n",
    "    \"\"\"\n",
    "    A local rule or quirk relevant to the person trying to sort their garbage.\n",
    "    Should include a concrete fine, specific sorting rule, collection day of the week, etc.\n",
    "    Should not include general information like \"fines may apply\" or \"state plans to enforce this law by 2025\".\n",
    "    Should not include rules that apply to facilities or businesses, only to individuals.\n",
    "    \"\"\"\n",
    "\n",
    "    description: Optional[str] = Field(\n",
    "        description=\"A description of the rule or quirk.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class SortingInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Information about how to sort a list of trash items.\n",
    "    \"\"\"\n",
    "\n",
    "    sorted_items: Optional[List[TrashItem]] = Field(\n",
    "        description=\"A list of trash items and their corresponding categories.\"\n",
    "    )\n",
    "    local_quirks: Optional[List[Quirk]] = Field(\n",
    "        description=\"Local quirks that need to be taken into account when sorting the items.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set up the prompt\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm designed to help with sorting garbage items based on local waste disposal rules. \"\n",
    "            \"You will receive a list of garbage item names and need to assign each one to its corresponding category according to the provided disposal rules. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, return null for the attribute's value.\",\n",
    "        ),\n",
    "        # â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“\n",
    "        MessagesPlaceholder(\"examples\"),  # <-- EXAMPLES!\n",
    "        # â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘\n",
    "        (\"human\", \"{rules}\\n===\\n{items}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# prompt.invoke(\n",
    "#     {\n",
    "#         \"rules\": \"some rules\",\n",
    "#         \"items\": \"this is items\",\n",
    "#         \"examples\": [HumanMessage(content=\"testing 1 2 3\")],\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Copy example-making code straight from LangChain docs\n",
    "\n",
    "import uuid\n",
    "from typing import Dict, List, TypedDict\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Example(TypedDict):\n",
    "    \"\"\"A representation of an example consisting of text input and expected tool calls.\n",
    "\n",
    "    For extraction, the tool calls are represented as instances of pydantic model.\n",
    "    \"\"\"\n",
    "\n",
    "    input: str  # This is the example text\n",
    "    tool_calls: List[BaseModel]  # Instances of pydantic model that should be extracted\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Example) -> List[BaseMessage]:\n",
    "    \"\"\"Convert an example into a list of messages that can be fed into an LLM.\n",
    "\n",
    "    This code is an adapter that converts our example to a list of messages\n",
    "    that can be fed into a chat model.\n",
    "\n",
    "    The list of messages per example corresponds to:\n",
    "\n",
    "    1) HumanMessage: contains the content from which content should be extracted.\n",
    "    2) AIMessage: contains the extracted information from the model\n",
    "    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.\n",
    "\n",
    "    The ToolMessage is required because some of the chat models are hyper-optimized for agents\n",
    "    rather than for an extraction use case.\n",
    "    \"\"\"\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"args\": tool_call.dict(),\n",
    "                # The name of the function right now corresponds\n",
    "                # to the name of the pydantic model\n",
    "                # This is implicit in the API right now,\n",
    "                # and will be improved over time.\n",
    "                \"name\": tool_call.__class__.__name__,\n",
    "            },\n",
    "        )\n",
    "    messages.append(AIMessage(content=\"\", tool_calls=tool_calls))\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"You have correctly called this tool.\"\n",
    "    ] * len(tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Add few-shot examples to make it easier for GPT-4o to understand the task\n",
    "\n",
    "with open(\"korea_summary.txt\", \"r\") as f:\n",
    "    example_rules = f.read()\n",
    "\n",
    "example_items = [\n",
    "    {\"item_id\": 0, \"class\": \"plastic\"},\n",
    "    {\"item_id\": 1, \"class\": \"paper\"},\n",
    "    {\"item_id\": 2, \"class\": \"foam\"},\n",
    "    {\"item_id\": 3, \"class\": \"food\"},\n",
    "]\n",
    "example_input = f\"{example_rules}\\n===\\n{example_items}\"\n",
    "\n",
    "\n",
    "example_output = SortingInfo(\n",
    "    sorted_items=[\n",
    "        TrashItem(item_id=\"0\", direction=\"Recyclable Waste (ìž¬í™œìš© ì“°ë ˆê¸°) (No special bags required)\"),\n",
    "        TrashItem(item_id=\"1\", direction=\"Recyclable Waste (ìž¬í™œìš© ì“°ë ˆê¸°) (No special bags required)\"),\n",
    "        TrashItem(item_id=\"2\", direction=\"General Waste Bag (ì¼ë°˜ ì“°ë ˆê¸° ë´‰íˆ¬)\"),\n",
    "        TrashItem(item_id=\"3\", direction=\"Food Waste Bag (ìŒì‹ë¬¼ ì“°ë ˆê¸° ë´‰íˆ¬)\"),\n",
    "    ],\n",
    "    local_quirks=[\n",
    "        Quirk(\n",
    "            description=\"Make sure to put items in appropriate garbage bags. Garbage bags are color-coded and district-specific. You can get them at local convenience stores.\"\n",
    "        ),\n",
    "        Quirk(description=\"Penalty for non-compliance is a fine of up to 300,000 KRW.\"),\n",
    "        Quirk(description=\"Do not put egg shells, seafood shells, and animal bones into the Food Waste Bag.\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    (\n",
    "        example_input,\n",
    "        example_output,\n",
    "    )\n",
    "]\n",
    "\n",
    "messages = []\n",
    "\n",
    "for text, tool_call in examples:\n",
    "    messages.extend(\n",
    "        tool_example_to_messages({\"input\": text, \"tool_calls\": [tool_call]})\n",
    "    )\n",
    "\n",
    "# example_prompt = prompt.invoke(\n",
    "#     {\"rules\": \"some rules\", \"items\": \"this is items\", \"examples\": messages}\n",
    "# )\n",
    "\n",
    "# for message in example_prompt.messages:\n",
    "#     print(f\"{message.type}: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Assemble the chain\n",
    "\n",
    "runnable = prompt | llm.with_structured_output(schema=SortingInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Load files with local rules\n",
    "\n",
    "rule_files = [\n",
    "    \"korea_summary.txt\",\n",
    "    \"california_summary.txt\",\n",
    "]\n",
    "\n",
    "local_rules = []\n",
    "for file in rule_files:\n",
    "    with open(file) as f:\n",
    "        local_rules.append(f.read())\n",
    "\n",
    "\n",
    "def get_random_rules():\n",
    "    return local_rules[random.randint(0, len(local_rules) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild segmentation dataset\n",
    "\n",
    "For this step, we're repurposing the TACO segmentation dataset.\n",
    "\n",
    "1. Draw ground-truth segmentation masks onto the images as outlines.\n",
    "2. Draw a number label inside each of the outlines. \n",
    "3. Provide class labels and rules in the text prompt.\n",
    "4. Generate a JSON target annotation using GPT-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.load_from_disk(\"taco_trash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Draw masks on the image as contours of random color\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import PIL\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "\n",
    "NUM_SAMPLES = 1000\n",
    "\n",
    "COLORS = [\n",
    "    (0, 255, 255),  # Cyan\n",
    "    (255, 128, 128),  # Salmon\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (255, 128, 0),  # Orange\n",
    "    (128, 255, 0),  # Lime\n",
    "    (0, 255, 128),  # Spring Green\n",
    "    (255, 0, 128),  # Rose\n",
    "    (128, 0, 255),  # Violet\n",
    "    (0, 128, 255),  # Azure\n",
    "    (128, 255, 128),  # Chartreuse\n",
    "    (128, 128, 255),  # Cornflower Blue\n",
    "    (255, 255, 128),  # Light Yellow\n",
    "    (255, 128, 255),  # Orchid\n",
    "    (128, 255, 255),  # Light Cyan\n",
    "    (255, 165, 0),  # Also orange\n",
    "    (0, 255, 255),  # Aqua\n",
    "    (255, 0, 255),  # Fuchsia\n",
    "    (128, 0, 0),  # Maroon\n",
    "    (128, 128, 0),  # Olive\n",
    "    (0, 128, 128),  # Teal\n",
    "    (128, 0, 128),  # Purple\n",
    "]\n",
    "\n",
    "colors = itertools.cycle(COLORS)\n",
    "\n",
    "\n",
    "def draw_contours(image, masks):\n",
    "    for j, mask in enumerate(masks):\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        image = cv2.drawContours(image, contours, -1, next(colors), 3)\n",
    "\n",
    "        if not contours:\n",
    "            continue\n",
    "\n",
    "        # Calculate the center of the outline\n",
    "        M = cv2.moments(contours[0])\n",
    "\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "        # Add a backdrop to the label for visibility\n",
    "        # text_size, _ = cv2.getTextSize(str(0), cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (cX - 5, cY - 30),\n",
    "            (cX + 30, cY + 5),\n",
    "            (0, 0, 0),  # Black color for the backdrop\n",
    "            -1,  # Fill the rectangle\n",
    "        )\n",
    "\n",
    "        # Draw the label with the index of the mask in the middle of the outline\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            str(j),  # Replace 0 with the index of the mask\n",
    "            (cX, cY),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),  # White color for the text\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use class labels and local rules to generate target JSONs\n",
    "\n",
    "def generate_annotations(seg_classes, original_classes):\n",
    "    items = [\n",
    "        {\"item_id\": i, \"class\": seg_class} for i, seg_class in enumerate(seg_classes)\n",
    "    ]\n",
    "    rules = get_random_rules()\n",
    "\n",
    "    sorting_info: SortingInfo = runnable.invoke(\n",
    "        {\"rules\": rules, \"items\": items, \"examples\": messages}\n",
    "    )\n",
    "\n",
    "    suffix_dict = {\n",
    "        \"sorted_items\": [],\n",
    "        \"local_quirks\": [quirk.dict() for quirk in sorting_info.local_quirks],\n",
    "    }\n",
    "\n",
    "    for item in sorting_info.sorted_items:\n",
    "        new_item = {\n",
    "            \"item_id\": item.item_id,\n",
    "            \"refined_label\": original_classes[int(item.item_id)],\n",
    "            \"direction\": item.direction,\n",
    "        }\n",
    "        suffix_dict[\"sorted_items\"].append(new_item)\n",
    "\n",
    "    prefix = {\n",
    "        \"rules\": rules,\n",
    "        \"items\": items,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"prefix\": prefix,\n",
    "        \"suffix\": suffix_dict,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Rebuild the dataset sample by sample\n",
    "\n",
    "tmp_path = Path(\"tmp_gpt_outputs.json\")\n",
    "tmp_path.touch(exist_ok=True)\n",
    "annotations_map = {}\n",
    "\n",
    "with tmp_path.open(\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        tmp_dict = json.loads(line)\n",
    "        annotations_map[tmp_dict[\"sample_index\"]] = tmp_dict[\"annotations\"]\n",
    "\n",
    "dataset_dict = defaultdict(list)\n",
    "\n",
    "for i, sample in tqdm(enumerate(ds)):\n",
    "    if i >= NUM_SAMPLES:\n",
    "        break\n",
    "\n",
    "    image = np.array(sample[\"image\"])\n",
    "    image = draw_contours(image, sample[\"masks\"])\n",
    "\n",
    "    if i in annotations_map:\n",
    "        annotations = annotations_map[i]\n",
    "    else:\n",
    "        annotations = generate_annotations(\n",
    "            sample[\"classes\"], sample[\"original_classes\"]\n",
    "        )\n",
    "        with tmp_path.open(\"a\") as f:\n",
    "            f.write(json.dumps({\"sample_index\": i, \"annotations\": annotations}) + \"\\n\")\n",
    "\n",
    "    # print(annotations[\"suffix\"])\n",
    "\n",
    "    dataset_dict[\"image\"].append(PIL.Image.fromarray(image))\n",
    "    dataset_dict[\"prefix\"].append(annotations[\"prefix\"])\n",
    "    dataset_dict[\"suffix\"].append(annotations[\"suffix\"])\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataset.save_to_disk(\"taco_trash_json_1K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "PIL.Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary bits from the toolkit\n",
    "# We are going to make a custom data preset for this one\n",
    "\n",
    "from training_toolkit import build_trainer, paligemma_image_preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_toolkit import DataPreset\n",
    "from training_toolkit.common.tokenization_utils.json import JSONTokenizer\n",
    "from string import Template\n",
    "\n",
    "# Define the template string\n",
    "PREFIX_TEMPLATE = Template(\"For every object outlined in the image, here're their detected classes: $items. \"\n",
    "                           \"For every outlined item, extract JSON with a more accurate label, \"\n",
    "                           \"as well as disposal directions based on local rules. \"\n",
    "                           \"The local rules are as follows: $rules.\")\n",
    "\n",
    "# Write a collator that is going to transform a list of samples out of dataset into input tensors for the model\n",
    "class TrashJSONCollator:\n",
    "\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.json_tokenizer = JSONTokenizer(processor)\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        json_dicts = [example[\"suffix\"] for example in examples]\n",
    "        labels = [self.json_tokenizer.encode(json_dict) for json_dict in json_dicts]\n",
    "\n",
    "        images = [example[\"image\"] for example in examples]\n",
    "\n",
    "        rules = [example[\"prefix\"][\"rules\"] for example in examples]\n",
    "        items = [example[\"prefix\"][\"items\"] for example in examples]\n",
    "        texts = [PREFIX_TEMPLATE.substitute(items=item, rules=rule)  for item, rule in zip(items, rules)]\n",
    "\n",
    "        tokens = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            suffix=labels,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "        )\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "# Plug the new collator into a data preset to make use of built-in utilities\n",
    "trash_json_preset = DataPreset(\n",
    "    train_test_split=0.2,\n",
    "    collator_cls=TrashJSONCollator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training params to fit the model into the memory\n",
    "\n",
    "paligemma_image_preset.training_args[\"per_device_train_batch_size\"] = 8\n",
    "paligemma_image_preset.training_args[\"per_device_eval_batch_size\"] = 8\n",
    "paligemma_image_preset.training_args[\"eval_strategy\"] = \"no\"\n",
    "paligemma_image_preset.training_args[\"num_train_epochs\"] = 10\n",
    "\n",
    "# Pass preconfigured components into the factory\n",
    "trainer = build_trainer( \n",
    "    **paligemma_image_preset.as_kwargs(),\n",
    "    **trash_json_preset.with_path(\"taco_trash_json_1K\").as_kwargs(apply_train_test_split=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoProcessor\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from training_toolkit.common.tokenization_utils.json import (\n",
    "    JSONTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"paligemma_2024-08-13_07-21-14/checkpoint-240\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(CHECKPOINT_PATH)\n",
    "processor = AutoProcessor.from_pretrained(CHECKPOINT_PATH)\n",
    "json_tokenizer = JSONTokenizer(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the inputs: a ruleset along with a list of segmented items\n",
    "\n",
    "from string import Template\n",
    "\n",
    "\n",
    "dataset = load_from_disk(\"taco_trash_json\")\n",
    "image = dataset[0][\"image\"]\n",
    "image\n",
    "\n",
    "rules = dataset[0][\"prefix\"][\"rules\"]\n",
    "items = dataset[0][\"prefix\"][\"items\"]\n",
    "\n",
    "PREFIX_TEMPLATE = Template(\"For every object outlined in the image, here're their detected classes: $items. \"\n",
    "                           \"For every outlined item, extract JSON with a more accurate label, \"\n",
    "                           \"as well as disposal directions based on local rules. \"\n",
    "                           \"The local rules are as follows: $rules.\")\n",
    "\n",
    "prompt = PREFIX_TEMPLATE.substitute(items=items, rules=rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text in a standard HF way\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "generated_text = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")[0]\n",
    "\n",
    "generated_json = json_tokenizer.decode(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
