{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional, List\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashItem(BaseModel):\n",
    "    \"\"\"\n",
    "    A trash item that needs to be sorted into a category.\n",
    "    \"\"\"\n",
    "\n",
    "    item_id: Optional[str] = Field(description=\"The unique identifier of the item.\")\n",
    "    direction: Optional[str] = Field(\n",
    "        description=\"How to dispose of the item according to the rules.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Quirk(BaseModel):\n",
    "    \"\"\"\n",
    "    A local rule or quirk relevant to the person trying to sort their garbage.\n",
    "    Should include a concrete fine, specific sorting rule, collection day of the week, etc.\n",
    "    Should not include general information like \"fines may apply\" or \"state plans to enforce this law by 2025\".\n",
    "    Should not include rules that apply to facilities or businesses, only to individuals.\n",
    "    \"\"\"\n",
    "\n",
    "    description: Optional[str] = Field(\n",
    "        description=\"A description of the rule or quirk.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class SortingInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Information about how to sort a list of trash items.\n",
    "    \"\"\"\n",
    "\n",
    "    sorted_items: Optional[List[TrashItem]] = Field(\n",
    "        description=\"A list of trash items and their corresponding categories.\"\n",
    "    )\n",
    "    local_quirks: Optional[List[Quirk]] = Field(\n",
    "        description=\"Local quirks that need to be taken into account when sorting the items.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm designed to help with sorting garbage items based on local waste disposal rules. \"\n",
    "            \"You will receive a list of garbage item names and need to assign each one to its corresponding category according to the provided disposal rules. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, return null for the attribute's value.\",\n",
    "        ),\n",
    "        # ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "        MessagesPlaceholder(\"examples\"),  # <-- EXAMPLES!\n",
    "        # ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "        (\"human\", \"{rules}\\n===\\n{items}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# prompt.invoke(\n",
    "#     {\n",
    "#         \"rules\": \"some rules\",\n",
    "#         \"items\": \"this is items\",\n",
    "#         \"examples\": [HumanMessage(content=\"testing 1 2 3\")],\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, List, TypedDict\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Example(TypedDict):\n",
    "    \"\"\"A representation of an example consisting of text input and expected tool calls.\n",
    "\n",
    "    For extraction, the tool calls are represented as instances of pydantic model.\n",
    "    \"\"\"\n",
    "\n",
    "    input: str  # This is the example text\n",
    "    tool_calls: List[BaseModel]  # Instances of pydantic model that should be extracted\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Example) -> List[BaseMessage]:\n",
    "    \"\"\"Convert an example into a list of messages that can be fed into an LLM.\n",
    "\n",
    "    This code is an adapter that converts our example to a list of messages\n",
    "    that can be fed into a chat model.\n",
    "\n",
    "    The list of messages per example corresponds to:\n",
    "\n",
    "    1) HumanMessage: contains the content from which content should be extracted.\n",
    "    2) AIMessage: contains the extracted information from the model\n",
    "    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.\n",
    "\n",
    "    The ToolMessage is required because some of the chat models are hyper-optimized for agents\n",
    "    rather than for an extraction use case.\n",
    "    \"\"\"\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"args\": tool_call.dict(),\n",
    "                # The name of the function right now corresponds\n",
    "                # to the name of the pydantic model\n",
    "                # This is implicit in the API right now,\n",
    "                # and will be improved over time.\n",
    "                \"name\": tool_call.__class__.__name__,\n",
    "            },\n",
    "        )\n",
    "    messages.append(AIMessage(content=\"\", tool_calls=tool_calls))\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"You have correctly called this tool.\"\n",
    "    ] * len(tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"korea_summary.txt\", \"r\") as f:\n",
    "    example_rules = f.read()\n",
    "\n",
    "example_items = [\n",
    "    {\"item_id\": 0, \"class\": \"plastic\"},\n",
    "    {\"item_id\": 1, \"class\": \"paper\"},\n",
    "    {\"item_id\": 2, \"class\": \"foam\"},\n",
    "    {\"item_id\": 3, \"class\": \"food\"},\n",
    "]\n",
    "example_input = f\"{example_rules}\\n===\\n{example_items}\"\n",
    "\n",
    "\n",
    "example_output = SortingInfo(\n",
    "    sorted_items=[\n",
    "        TrashItem(item_id=\"0\", direction=\"Recyclable Waste (재활용 쓰레기) (No special bags required)\"),\n",
    "        TrashItem(item_id=\"1\", direction=\"Recyclable Waste (재활용 쓰레기) (No special bags required)\"),\n",
    "        TrashItem(item_id=\"2\", direction=\"General Waste Bag (일반 쓰레기 봉투)\"),\n",
    "        TrashItem(item_id=\"3\", direction=\"Food Waste Bag (음식물 쓰레기 봉투)\"),\n",
    "    ],\n",
    "    local_quirks=[\n",
    "        Quirk(\n",
    "            description=\"Make sure to put items in appropriate garbage bags. Garbage bags are color-coded and district-specific. You can get them at local convenience stores.\"\n",
    "        ),\n",
    "        Quirk(description=\"Penalty for non-compliance is a fine of up to 300,000 KRW.\"),\n",
    "        Quirk(description=\"Do not put egg shells, seafood shells, and animal bones into the Food Waste Bag.\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    (\n",
    "        example_input,\n",
    "        example_output,\n",
    "    )\n",
    "]\n",
    "\n",
    "messages = []\n",
    "\n",
    "for text, tool_call in examples:\n",
    "    messages.extend(\n",
    "        tool_example_to_messages({\"input\": text, \"tool_calls\": [tool_call]})\n",
    "    )\n",
    "\n",
    "# example_prompt = prompt.invoke(\n",
    "#     {\"rules\": \"some rules\", \"items\": \"this is items\", \"examples\": messages}\n",
    "# )\n",
    "\n",
    "# for message in example_prompt.messages:\n",
    "#     print(f\"{message.type}: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | llm.with_structured_output(schema=SortingInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_files = [\n",
    "    \"korea_summary.txt\",\n",
    "    \"california_summary.txt\",\n",
    "]\n",
    "\n",
    "local_rules = []\n",
    "for file in rule_files:\n",
    "    with open(file) as f:\n",
    "        local_rules.append(f.read())\n",
    "\n",
    "\n",
    "def get_random_rules():\n",
    "    return local_rules[random.randint(0, len(local_rules) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.load_from_disk(\"taco_trash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import supervision as sv\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import PIL\n",
    "import random\n",
    "\n",
    "\n",
    "NUM_SAMPLES = 100\n",
    "\n",
    "COLORS = [\n",
    "    (0, 255, 255),  # Cyan\n",
    "    (255, 128, 128),  # Salmon\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (255, 128, 0),  # Orange\n",
    "    (128, 255, 0),  # Lime\n",
    "    (0, 255, 128),  # Spring Green\n",
    "    (255, 0, 128),  # Rose\n",
    "    (128, 0, 255),  # Violet\n",
    "    (0, 128, 255),  # Azure\n",
    "    (128, 255, 128),  # Chartreuse\n",
    "    (128, 128, 255),  # Cornflower Blue\n",
    "    (255, 255, 128),  # Light Yellow\n",
    "    (255, 128, 255),  # Orchid\n",
    "    (128, 255, 255),  # Light Cyan\n",
    "    (255, 0, 0),  # Red\n",
    "    (0, 255, 0),  # Green\n",
    "    (0, 0, 255),  # Blue\n",
    "    (255, 165, 0),  # Orange\n",
    "    (0, 255, 255),  # Aqua\n",
    "    (255, 0, 255),  # Fuchsia\n",
    "    (128, 0, 0),  # Maroon\n",
    "]\n",
    "\n",
    "\n",
    "def draw_contours(image, masks):\n",
    "    selected_colors = random.sample(COLORS, len(masks))\n",
    "\n",
    "    for j, mask in enumerate(masks):\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        image = cv2.drawContours(image, contours, -1, selected_colors[j], 3)\n",
    "\n",
    "        if not contours:\n",
    "            continue\n",
    "\n",
    "        # Calculate the center of the outline\n",
    "        M = cv2.moments(contours[0])\n",
    "\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "        # Add a backdrop to the label for visibility\n",
    "        text_size, _ = cv2.getTextSize(str(0), cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (cX - 5, cY - 30),\n",
    "            (cX + 30, cY + 5),\n",
    "            (0, 0, 0),  # Black color for the backdrop\n",
    "            -1,  # Fill the rectangle\n",
    "        )\n",
    "\n",
    "        # Draw the label with the index of the mask in the middle of the outline\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            str(j),  # Replace 0 with the index of the mask\n",
    "            (cX, cY),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),  # White color for the text\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "    return image\n",
    "\n",
    "\n",
    "def generate_annotations(seg_classes, original_classes):\n",
    "    items = [\n",
    "        {\"item_id\": i, \"class\": seg_class} for i, seg_class in enumerate(seg_classes)\n",
    "    ]\n",
    "    rules = get_random_rules()\n",
    "\n",
    "    sorting_info: SortingInfo = runnable.invoke(\n",
    "        {\"rules\": rules, \"items\": items, \"examples\": messages}\n",
    "    )\n",
    "\n",
    "    suffix_dict = {\n",
    "        \"sorted_items\": [],\n",
    "        \"local_quirks\": [quirk.dict() for quirk in sorting_info.local_quirks],\n",
    "    }\n",
    "\n",
    "    for item in sorting_info.sorted_items:\n",
    "        new_item = {\n",
    "            \"item_id\": item.item_id,\n",
    "            \"refined_label\": original_classes[int(item.item_id)],\n",
    "            \"direction\": item.direction,\n",
    "        }\n",
    "        suffix_dict[\"sorted_items\"].append(new_item)\n",
    "\n",
    "    prefix = {\n",
    "        \"rules\": rules,\n",
    "        \"items\": items,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"prefix\": prefix,\n",
    "        \"suffix\": suffix_dict,\n",
    "    }\n",
    "\n",
    "\n",
    "dataset_dict = defaultdict(list)\n",
    "\n",
    "for i, sample in tqdm(enumerate(ds)):\n",
    "    if i > NUM_SAMPLES:\n",
    "        break\n",
    "\n",
    "    image = np.array(sample[\"image\"])\n",
    "    image = draw_contours(image, sample[\"masks\"])\n",
    "\n",
    "    annotations = generate_annotations(sample[\"classes\"], sample[\"original_classes\"])\n",
    "\n",
    "    dataset_dict[\"image\"].append(PIL.Image.fromarray(image))\n",
    "    dataset_dict[\"prefix\"].append(annotations[\"prefix\"])\n",
    "    dataset_dict[\"suffix\"].append(annotations[\"suffix\"])\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataset.save_to_disk(\"taco_trash_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "PIL.Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_toolkit import build_trainer, paligemma_image_preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_toolkit import DataPreset\n",
    "from training_toolkit.common.tokenization_utils.json import JSONTokenizer\n",
    "from string import Template\n",
    "\n",
    "# Define the template string\n",
    "PREFIX_TEMPLATE = Template(\"For every object outlined in the image, here're their detected classes: $items. \"\n",
    "                           \"For every outlined item, extract JSON with a more accurate label, \"\n",
    "                           \"as well as disposal directions based on local rules. \"\n",
    "                           \"The local rules are as follows: $rules.\")\n",
    "\n",
    "\n",
    "class TrashJSONCollator:\n",
    "\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.json_tokenizer = JSONTokenizer(processor)\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        json_dicts = [example[\"suffix\"] for example in examples]\n",
    "        labels = [self.json_tokenizer.encode(json_dict) for json_dict in json_dicts]\n",
    "\n",
    "        images = [example[\"image\"] for example in examples]\n",
    "\n",
    "        rules = [example[\"prefix\"][\"rules\"] for example in examples]\n",
    "        items = [example[\"prefix\"][\"items\"] for example in examples]\n",
    "        texts = [PREFIX_TEMPLATE.substitute(items=item, rules=rule)  for item, rule in zip(items, rules)]\n",
    "\n",
    "        tokens = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            suffix=labels,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "        )\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "\n",
    "trash_json_preset = DataPreset(\n",
    "    train_test_split=0.2,\n",
    "    collator_cls=TrashJSONCollator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoProcessor\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"google/paligemma-3b-mix-224\")\n",
    "\n",
    "# prepared_datasets = trash_json_preset.with_path(\"taco_trash_json\").as_kwargs()\n",
    "# train_dataset = prepared_datasets[\"train_dataset\"]\n",
    "\n",
    "# collator = prepared_datasets[\"collator_cls\"](processor)\n",
    "\n",
    "# collator([train_dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paligemma_image_preset.training_args[\"per_device_train_batch_size\"] = 10\n",
    "paligemma_image_preset.training_args[\"per_device_eval_batch_size\"] = 10\n",
    "paligemma_image_preset.training_args[\"num_train_epochs\"] = 2\n",
    "\n",
    "trainer = build_trainer(\n",
    "    **paligemma_image_preset.as_kwargs(),\n",
    "    **trash_json_preset.with_path(\"taco_trash_json\").as_kwargs()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
