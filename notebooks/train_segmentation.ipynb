{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦¾ Training Toolkit: Segmentation\n",
    "\n",
    "## 1. Preparations\n",
    "\n",
    "### Download data and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O taco.zip https://zenodo.org/records/3587843/files/TACO.zip?download=1\n",
    "!unzip -o taco.zip -d taco_raw && rm taco.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && pip3 install -r requirements.txt\n",
    "!cd .. && git clone https://github.com/tensorsense/training_toolkit.git\n",
    "!cd ../training_toolkit && pip3 install --upgrade -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataset into HF ðŸ¤— format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Image\n",
    "from collections import defaultdict\n",
    "import PIL\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"taco_raw/TACO/data/\")\n",
    "coco = COCO(dataset_path.joinpath(\"annotations.json\").as_posix())\n",
    "\n",
    "image_ids = coco.getImgIds()\n",
    "categories = [coco.cats[cat_id][\"name\"] for cat_id in coco.getCatIds()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map some of the classes to a more general class to make segmentation easier\n",
    "\n",
    "plastic = [\n",
    "    \"Other plastic bottle\",\n",
    "    \"Clear plastic bottle\",\n",
    "    \"Plastic bottle cap\",\n",
    "    \"Disposable plastic cup\",\n",
    "    \"Other plastic cup\",\n",
    "    \"Plastic lid\",\n",
    "    \"Other plastic\",\n",
    "    \"Plastic film\",\n",
    "    \"Other plastic wrapper\",\n",
    "    \"Other plastic container\",\n",
    "    \"Plastic glooves\",\n",
    "    \"Plastic utensils\",\n",
    "    \"Plastic straw\",\n",
    "    \"Disposable food container\",\n",
    "    \"Polypropylene bag\",\n",
    "    \"Single-use carrier bag\",\n",
    "    \"Carded blister pack\",\n",
    "    \"Crisp packet\",\n",
    "    \"Garbage bag\",\n",
    "    \"Six pack rings\",\n",
    "    \"Spread tub\",\n",
    "    \"Squeezable tube\",\n",
    "    \"Tupperware\",\n",
    "]\n",
    "\n",
    "glass = [\"Glass bottle\", \"Broken glass\", \"Glass cup\", \"Glass jar\"]\n",
    "\n",
    "paper = [\n",
    "    \"Paper cup\",\n",
    "    \"Magazine paper\",\n",
    "    \"Wrapping paper\",\n",
    "    \"Normal paper\",\n",
    "    \"Paper bag\",\n",
    "    \"Plastified paper bag\",\n",
    "    \"Paper straw\",\n",
    "]\n",
    "\n",
    "carton = [\n",
    "    \"Other carton\",\n",
    "    \"Egg carton\",\n",
    "    \"Drink carton\",\n",
    "    \"Corrugated carton\",\n",
    "    \"Meal carton\",\n",
    "    \"Pizza box\",\n",
    "    \"Toilet tube\",\n",
    "]\n",
    "\n",
    "metal = [\n",
    "    \"Aluminium foil\",\n",
    "    \"Aluminium blister pack\",\n",
    "    \"Metal bottle cap\",\n",
    "    \"Food Can\",\n",
    "    \"Drink can\",\n",
    "    \"Metal lid\",\n",
    "    \"Scrap metal\",\n",
    "    \"Pop tab\",\n",
    "]\n",
    "\n",
    "foam = [\n",
    "    \"Foam cup\",\n",
    "    \"Foam food container\",\n",
    "    \"Styrofoam piece\",\n",
    "]\n",
    "\n",
    "special = [\n",
    "    \"Aerosol\",\n",
    "    \"Battery\",\n",
    "    \"Rope & strings\",\n",
    "    \"Shoe\",\n",
    "    \"Cigarette\",\n",
    "]\n",
    "\n",
    "food = [\n",
    "    \"Food waste\",\n",
    "]\n",
    "\n",
    "general = [\n",
    "    \"Tissues\",\n",
    "    \"Unlabeled litter\",\n",
    "]\n",
    "\n",
    "class_map = (\n",
    "    {item: \"plastic\" for item in plastic}\n",
    "    | {item: \"glass\" for item in glass}\n",
    "    | {item: \"paper\" for item in paper}\n",
    "    | {item: \"carton\" for item in carton}\n",
    "    | {item: \"metal\" for item in metal}\n",
    "    | {item: \"foam\" for item in foam}\n",
    "    | {item: \"special\" for item in special}\n",
    "    | {item: \"food\" for item in food}\n",
    "    | {item: \"general\" for item in general}\n",
    ")\n",
    "\n",
    "class_names = set(class_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512\n",
    "LIMIT_SAMPLES = 500\n",
    "\n",
    "# HF Datasets may choke on full images, so we'll resize them to a smaller size\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=IMAGE_SIZE, always_apply=True),\n",
    "        A.CenterCrop(height=IMAGE_SIZE, width=IMAGE_SIZE, always_apply=True),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\", label_fields=[\"class_labels\"], clip=True, min_area=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "dataset_dict = defaultdict(list)\n",
    "prefix = \"segment \" + \" ; \".join(class_names)\n",
    "\n",
    "\n",
    "for image_id in tqdm(image_ids):\n",
    "\n",
    "    # 1. Parse COCO annotations\n",
    "    image_path = dataset_path.joinpath(coco.loadImgs(image_id)[0][\"file_name\"])\n",
    "    annotations = coco.loadAnns(coco.getAnnIds(image_id))\n",
    "    xywh_bboxes = [ann[\"bbox\"] for ann in annotations]\n",
    "    xyxy_bboxes = [[x, y, x + w, y + h] for x, y, w, h in xywh_bboxes]\n",
    "    original_classes = [categories[ann[\"category_id\"]] for ann in annotations]\n",
    "    classes = [class_map[original_class] for original_class in original_classes]\n",
    "\n",
    "    # 2. Load and resize the image and its annotations\n",
    "    image = cv2.imread(image_path.as_posix())\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    masks = [coco.annToMask(ann) for ann in annotations]\n",
    "\n",
    "    transformed = transform(\n",
    "        image=image, masks=masks, bboxes=xyxy_bboxes, class_labels=classes\n",
    "    )\n",
    "\n",
    "    # 3. Prepare the sample for storage\n",
    "    image = PIL.Image.fromarray(transformed[\"image\"])\n",
    "    masks = np.array(transformed[\"masks\"], dtype=bool)\n",
    "    xyxy_bboxes = np.array(transformed[\"bboxes\"], dtype=int)\n",
    "    classes = transformed[\"class_labels\"]\n",
    "\n",
    "    xyxy_bboxes = np.array(\n",
    "        [\n",
    "            [x1, y1, x2, y2]\n",
    "            for x1, y1, x2, y2 in xyxy_bboxes\n",
    "            if x2 - x1 > 0 and y2 - y1 > 0\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if len(masks) == 0 or len(xyxy_bboxes) == 0 or len(classes) == 0:\n",
    "        continue\n",
    "\n",
    "    assert len(masks.shape) == 3\n",
    "    assert (\n",
    "        len(xyxy_bboxes.shape) == 2 and xyxy_bboxes.shape[1] == 4\n",
    "    ), f\"{xyxy_bboxes.shape}, {len(masks)}, {len(xyxy_bboxes)}\"\n",
    "\n",
    "    # 4. Store the sample\n",
    "    dataset_dict[\"image\"].append(image)\n",
    "    dataset_dict[\"prompt\"].append(prefix)\n",
    "    dataset_dict[\"xyxy_bboxes\"].append(xyxy_bboxes)\n",
    "    dataset_dict[\"masks\"].append(masks)\n",
    "    dataset_dict[\"classes\"].append(classes)\n",
    "    dataset_dict[\"original_classes\"].append(original_classes)\n",
    "\n",
    "\n",
    "# Convert the dataset to HF format and save it to disk\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataset = dataset.cast_column(\"image\", Image())\n",
    "\n",
    "dataset.info.dataset_name = \"taco_trash\"\n",
    "dataset.info.description = f\"class_names: {' ; '.join(class_names)}\"\n",
    "\n",
    "dataset.save_to_disk(\"taco_trash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test segmentation tokenizer\n",
    "\n",
    "`SegmentationTokenizer` is a utility that transforms segmentation masks into sequences of 20 tokens and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import PIL\n",
    "import numpy as np\n",
    "from training_toolkit.common.tokenization_utils.segmentation import (\n",
    "    SegmentationTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load_from_disk(\"taco_trash\")\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "\n",
    "segmentation_tokenizer = SegmentationTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Let's take a look at the original image\n",
    "example = dataset[0]\n",
    "PIL.Image.fromarray(example[\"image\"].permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ...and it's mask\n",
    "PIL.Image.fromarray(example[\"masks\"][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Now let's encode the mask and take a look at the resulting token\n",
    "\n",
    "suffix = segmentation_tokenizer.encode(\n",
    "    example[\"image\"], example[\"xyxy_bboxes\"], example[\"masks\"], example[\"classes\"]\n",
    ")\n",
    "\n",
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Finally, let's decode the token sequence back into a pixel-level mask again\n",
    "decoded = segmentation_tokenizer.decode(suffix, 512, 512)\n",
    "\n",
    "PIL.Image.fromarray((decoded[0][\"mask\"] > 0.5).astype(np.uint8) * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PaliGemma is in the gated repo, so we need to load the HF API token\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary bits from the toolkit\n",
    "from training_toolkit import paligemma_image_preset, image_segmentation_preset, build_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default setup results in OOM, so we need to set a smaller batch size\n",
    "paligemma_image_preset.training_args[\"per_device_train_batch_size\"] = 12\n",
    "paligemma_image_preset.training_args[\"per_device_eval_batch_size\"] = 12\n",
    "paligemma_image_preset.training_args[\"num_train_epochs\"] = 8\n",
    "\n",
    "# Pass necessary arguments to the trainer (most of them pre-made in the presets)\n",
    "trainer = build_trainer(\n",
    "    **paligemma_image_preset.as_kwargs(),\n",
    "    **image_segmentation_preset.with_path(\"taco_trash\").as_kwargs()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoProcessor\n",
    "import PIL\n",
    "import numpy as np\n",
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "from training_toolkit.common.tokenization_utils.segmentation import (\n",
    "    SegmentationTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"paligemma_2024-08-06_09-05-06\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(CHECKPOINT_PATH)\n",
    "processor = AutoProcessor.from_pretrained(CHECKPOINT_PATH)\n",
    "segmentation_tokenizer = SegmentationTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(\"../assets/trash1.jpg\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. In order to be in line with pretraining, we need to pass class names as part of the prompt\n",
    "PROMPT = \"segment \" + \" ; \".join(class_names)\n",
    "\n",
    "# 2. Let's generate some text using the standard HF way\n",
    "inputs = processor(images=image, text=PROMPT)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "\n",
    "# 3. Chop up the result to recover generated segmentation masks\n",
    "image_token_index = model.config.image_token_index\n",
    "num_image_tokens = len(generated_ids[generated_ids == image_token_index])\n",
    "num_text_tokens = len(processor.tokenizer.encode(PROMPT))\n",
    "num_prompt_tokens = num_image_tokens + num_text_tokens + 2\n",
    "\n",
    "# 4. Decode and reconstruct the masks\n",
    "generated_text = processor.batch_decode(\n",
    "    generated_ids[:, num_prompt_tokens:],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")[0]\n",
    "\n",
    "w, h = image.size\n",
    "\n",
    "generated_segmentation = segmentation_tokenizer.decode(generated_text, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.fromarray((generated_segmentation[0][\"mask\"] > 0.5).astype(np.uint8) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyxy = []\n",
    "mask = []\n",
    "class_id = []\n",
    "class_name = []\n",
    "\n",
    "for r in generated_segmentation:\n",
    "    if \"xyxy\" not in r or \"mask\" not in r or r[\"mask\"] is None:\n",
    "        continue\n",
    "    \n",
    "    xyxy.append(r[\"xyxy\"])\n",
    "    _, m = cv2.threshold(r[\"mask\"], 0.5, 1.0, cv2.THRESH_BINARY)\n",
    "    mask.append(m)\n",
    "    # class_id.append(ds.classes.index(r[\"name\"].strip()))\n",
    "    # class_id.append(classes.index(r['name'].strip()))\n",
    "    class_id.append(list(class_names).index(r[\"name\"].strip()))\n",
    "    class_name.append(r[\"name\"].strip() if r[\"name\"] is not None else \"trash\")\n",
    "\n",
    "detections = sv.Detections(\n",
    "    xyxy=np.array(xyxy).astype(int),\n",
    "    mask=np.array(mask).astype(bool),\n",
    "    class_id=np.array(class_id).astype(int),\n",
    ")\n",
    "\n",
    "detections[\"class_name\"] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = sv.BoxAnnotator().annotate(image, detections)\n",
    "\n",
    "image = sv.MaskAnnotator().annotate(image, detections)\n",
    "image = sv.LabelAnnotator(text_scale=2, text_thickness=4, text_position=sv.Position.CENTER_OF_MASS, text_color=sv.Color.BLACK).annotate(image, detections)\n",
    "\n",
    "# sv.plot_images_grid([image], (2, 2))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
